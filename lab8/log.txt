I started the lab by checking if my version of GNU coreutils was up to date with
> sort --version
I have coreutils 8.24 which works for the purposes of the lab.

I then changed the default path to /usr/local/cs/bin using
> export PATH=/usr/local/cs/bin:$PATH

Next, I generated a file containing at least 10,000,000 random double-precision floating point numbers, one per line with no white space. The od command paired with the -t f flags is helpful here sinc
e this outputs double-precision floating point values. I created a file of 80,000,000 bytes since each double uses 8 bytes and fed the od command input from /dev/urandom. I finally piped this to a tra
nslate command which replaced all spaces with new lines. The entire command ended up looking like
> od -t f -N 80000000 < /dev/urandom | tr -s ' ' '\n' > randomdoubles.txt

Now, I can use the time -p on the sort command with the -g flag to measure how long it takes to sort my doubles file, then send the output to /dev/null.
> time -p sort -g randomdoubles.txt > /dev/null

This outputs:
real 42.57
user 229.02
sys 0.58

Next, I want to observe the efficiency of using more threads by using the --parallel option. In total, I record the times using 1, 2, 4, and 8 threads.

1 thread
> time -p sort -g --parallel=1 randomdoubles.txt > /dev/null
real 214.87
user 214.62
sys 0.26


2 threads
> time -p sort -g --parallel=2 randomdoubles.txt > /dev/null
real 113.32
user 216.89
sys 0.30


4 threads
> time -p sort -g --parallel=4 randomdoubles.txt > /dev/null
real 63.90
user 218.64
sys 0.40


8 threads
> time -p sort -g --parallel=8 randomdoubles.txt > /dev/null
real 42.25
user 228.63
sys 0.54

As apparent from the results, using more threads leads to less time intensive commands, proving that parallelism is an effective way of increasing a program's efficiency. 
